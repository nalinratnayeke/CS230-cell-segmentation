{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fy_FqY_TDlE6"
   },
   "source": [
    "### Reqirements\n",
    "- keras >= 2.2.0 or tensorflow >= 1.13\n",
    "- segmenation-models==1.0.*\n",
    "- albumentations==0.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCWcEBobDlFA"
   },
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18284,
     "status": "ok",
     "timestamp": 1582585515629,
     "user": {
      "displayName": "Yilin Fan",
      "photoUrl": "",
      "userId": "03012599210889850416"
     },
     "user_tz": 480
    },
    "id": "BOSyTNIUDlFC",
    "outputId": "aa89d9b4-0314-4d30-c590-f32cf6d820d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#!pip install tiffile --quiet\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import glob\n",
    "from tiffile import imsave\n",
    "\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKC_AxGnDlFI"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/data/'\n",
    "\n",
    "# Debug for fileloading\n",
    "FLAG_DEBUG_LOADING = 0\n",
    "\n",
    "# Single z stack\n",
    "FLAG_SINGLE_Z = 1\n",
    "FLAG_3CLASS = 1\n",
    "\n",
    "# Convert image to imtype \n",
    "imtype = \"uint8\"\n",
    "# imtype = \"float32\"\n",
    "\n",
    "class_weights=np.array([0 , 1 , .5])\n",
    "class_labels = ['background' , 'nucleus' , 'cytoplasm']\n",
    "CLASSES = class_labels\n",
    "\n",
    "\n",
    "dir_tag = ''\n",
    "tag = ''\n",
    "\n",
    "x_train_dir = [os.path.join(DATA_DIR, 'Image_BF1_train') , os.path.join(DATA_DIR, 'Image_BF2_train') , os.path.join(DATA_DIR, 'Image_BF3_train')]\n",
    "y_train_dir = os.path.join(DATA_DIR, 'Mask_3class_train' + dir_tag)\n",
    "\n",
    "x_valid_dir = [os.path.join(DATA_DIR, 'Image_BF1_dev') , os.path.join(DATA_DIR, 'Image_BF2_dev') , os.path.join(DATA_DIR, 'Image_BF3_dev')]\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'Mask_3class_dev' + dir_tag)\n",
    "\n",
    "x_test_dir = [os.path.join(DATA_DIR, 'Image_BF1_test') , os.path.join(DATA_DIR, 'Image_BF2_test') , os.path.join(DATA_DIR, 'Image_BF3_test')]\n",
    "y_test_dir = os.path.join(DATA_DIR, 'Mask_3class_test' + dir_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GgYAZKGDlFL"
   },
   "source": [
    "# Dataloader and utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2achLqHwDlFM"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(50, 50))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, np.ceil(n / 1), i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = class_labels\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            n_images=None\n",
    "    ):\n",
    "        # Make sure files are present in all folders\n",
    "        self.ids1 = [os.path.basename(x) for x in glob.glob(images_dir[1] + '/*.tif')]\n",
    "        self.ids3 = [os.path.basename(x) for x in glob.glob(masks_dir + '/*.tif')]\n",
    "        self.ids = list(set(self.ids1) & set(self.ids3))\n",
    "\n",
    "        # # Number of training examples\n",
    "        if len(self.ids) > n_images:\n",
    "            self.ids = self.ids[0:n_images]\n",
    "\n",
    "        self.images_fps0 = [os.path.join(images_dir[0], image_id) for image_id in self.ids]\n",
    "        self.images_fps1 = [os.path.join(images_dir[1], image_id) for image_id in self.ids]\n",
    "        self.images_fps2 = [os.path.join(images_dir[2], image_id) for image_id in self.ids]\n",
    "\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        if FLAG_DEBUG_LOADING == 1:\n",
    "            print(self.masks_fps[i])\n",
    "            \n",
    "        # read data and convert\n",
    "        if FLAG_SINGLE_Z == 0:\n",
    "            image0 = cv2.imread(self.images_fps0[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image1 = cv2.imread(self.images_fps1[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image2 = cv2.imread(self.images_fps2[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "        else:\n",
    "            image1 = cv2.imread(self.images_fps1[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image0 = image1\n",
    "            image2 = image1\n",
    "        image = np.stack((image0 , image1 , image2) , axis = 2)\n",
    "\n",
    "        if imtype == \"uint8\":\n",
    "            image = cv2.convertScaleAbs(image , alpha = (255. / 65535.))\n",
    "        if imtype == \"float32\":\n",
    "            image = image.astype(\"float32\")\n",
    "\n",
    "        # Resize image\n",
    "        # print(image)\n",
    "        if FLAG_RESIZE == 1:\n",
    "            image = cv2.resize(image , (imsize , imsize))\n",
    "\n",
    "        if FLAG_DEBUG_LOADING == 1:\n",
    "            print(self.masks_fps[i])\n",
    "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "        # Resize mask using nearest neighbor to avoid decimal points\n",
    "        if FLAG_RESIZE == 1:\n",
    "            mask = cv2.resize(mask , dsize = (imsize , imsize) , interpolation = cv2.INTER_NEAREST)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        # print(self.class_values)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary: not necessary since every pixel is labeled\n",
    "        # if mask.shape[-1] != 1:\n",
    "        #     background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "        #     mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)   \n",
    "            \n",
    "            \n",
    "### Augmentations\n",
    "# def round_clip_0_1(x, **kwargs):\n",
    "#     return x.round().clip(0, 1)\n",
    "\n",
    "# define heavy augmentations\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.VerticalFlip(p=0.5),\n",
    "\n",
    "        A.Transpose(p=0.5),\n",
    "\n",
    "        # A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=imsize, min_width=imsize, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=imsize, width=imsize, always_apply=True),\n",
    "\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(imsize_test, imsize_test)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:models_vgg16_16_640_4_3750_2020-03-07 01:22:57.127240\n",
      "1:models_vgg16_128_640_4_468_2020-03-07 05:55:33.992073\n",
      "2:models_vgg16_512_640_4_117_2020-03-07 10:21:15.504758\n",
      "3:models_vgg16_1024_640_4_58_2020-03-07 14:44:55.288484\n",
      "4:models_vgg16_1500_640_4_40_2020-03-07 19:06:47.991860\n",
      "5:models_efficientnetb4_16_320_8_3750_2020-03-08 00:24:44.388484\n",
      "6:models_efficientnetb4_128_320_8_468_2020-03-08 04:53:37.271104\n",
      "7:models_efficientnetb4_512_320_8_117_2020-03-08 09:16:00.047692\n",
      "8:models_efficientnetb4_1024_320_8_58_2020-03-08 13:40:57.329230\n",
      "9:models_efficientnetb4_1500_320_8_40_2020-03-08 18:06:30.098173\n",
      "10:models_focal_vgg16_1500_640_4_40_2020-03-09 17:32:52.239663\n",
      "11:models_dice_vgg16_1500_640_4_40_2020-03-09 18:48:40.628932\n",
      "12:models_fullval_vgg16_1500_640_4_40_2020-03-09 21:16:35.334652\n",
      "13:models_fullval_efficientnetb4_1500_320_8_40_2020-03-09 23:10:08.517890\n",
      "14:models_4class_vgg16_1500_640_4_40_2020-03-10 22:02:04.105286\n"
     ]
    }
   ],
   "source": [
    "### other stuff\n",
    "# Random crops\n",
    "FLAG_RESIZE = 0\n",
    "LR = 0.0001\n",
    "imsize_test = 2176\n",
    "n_images_dev = 150\n",
    "n_images_test = 150\n",
    "n_images_train = 1500\n",
    "imsize = 640\n",
    "BATCH = 4\n",
    "\n",
    "# define network parameters\n",
    "n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES))  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "# load best weights\n",
    "from keras.models import load_model\n",
    "\n",
    "model_dir = '/data/models'\n",
    "model_filenames = os.listdir(model_dir)\n",
    "# print(model_filenames)\n",
    "for temp in range(len(model_filenames)): \n",
    "    print(str(temp) + ':' + model_filenames[temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wIlc_0wDlFx"
   },
   "source": [
    "# Model Evaluation VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BS6H1vZZDlFh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_vgg16_16_640_4_3750_2020-03-07 01:22:57.127240\n",
      "hybrid loss\n",
      "150/150 [==============================] - 63s 420ms/step\n",
      "Loss: 0.20237\n",
      "mean iou_score: 0.74797\n",
      "mean f1-score: 0.83107\n",
      "150/150 [==============================] - 63s 420ms/step\n",
      "Loss: 0.20237\n",
      "nuclear iou_score: 0.7471\n",
      "nuclear f1-score: 0.85455\n",
      "models_vgg16_128_640_4_468_2020-03-07 05:55:33.992073\n",
      "hybrid loss\n",
      "150/150 [==============================] - 63s 422ms/step\n",
      "Loss: 0.1862\n",
      "mean iou_score: 0.76683\n",
      "mean f1-score: 0.84199\n",
      "150/150 [==============================] - 63s 422ms/step\n",
      "Loss: 0.1862\n",
      "nuclear iou_score: 0.78066\n",
      "nuclear f1-score: 0.8762\n",
      "models_vgg16_512_640_4_117_2020-03-07 10:21:15.504758\n",
      "hybrid loss\n",
      "150/150 [==============================] - 65s 435ms/step\n",
      "Loss: 0.18516\n",
      "mean iou_score: 0.77483\n",
      "mean f1-score: 0.84689\n",
      "150/150 [==============================] - 64s 429ms/step\n",
      "Loss: 0.18516\n",
      "nuclear iou_score: 0.79436\n",
      "nuclear f1-score: 0.88487\n",
      "models_vgg16_1024_640_4_58_2020-03-07 14:44:55.288484\n",
      "hybrid loss\n",
      "150/150 [==============================] - 65s 435ms/step\n",
      "Loss: 0.1768\n",
      "mean iou_score: 0.77586\n",
      "mean f1-score: 0.8474\n",
      "150/150 [==============================] - 65s 435ms/step\n",
      "Loss: 0.1768\n",
      "nuclear iou_score: 0.79679\n",
      "nuclear f1-score: 0.88643\n",
      "models_vgg16_1500_640_4_40_2020-03-07 19:06:47.991860\n",
      "hybrid loss\n",
      "150/150 [==============================] - 65s 435ms/step\n",
      "Loss: 0.17359\n",
      "mean iou_score: 0.77274\n",
      "mean f1-score: 0.84529\n",
      "150/150 [==============================] - 66s 438ms/step\n",
      "Loss: 0.17359\n",
      "nuclear iou_score: 0.79598\n",
      "nuclear f1-score: 0.88597\n",
      "models_efficientnetb4_16_320_8_3750_2020-03-08 00:24:44.388484\n",
      "hybrid loss\n",
      "150/150 [==============================] - 72s 480ms/step\n",
      "Loss: 0.25235\n",
      "mean iou_score: 0.73639\n",
      "mean f1-score: 0.82297\n",
      "150/150 [==============================] - 70s 465ms/step\n",
      "Loss: 0.25235\n",
      "nuclear iou_score: 0.72782\n",
      "nuclear f1-score: 0.84133\n",
      "models_efficientnetb4_128_320_8_468_2020-03-08 04:53:37.271104\n",
      "hybrid loss\n",
      "150/150 [==============================] - 70s 465ms/step\n",
      "Loss: 0.20554\n",
      "mean iou_score: 0.74786\n",
      "mean f1-score: 0.83045\n",
      "150/150 [==============================] - 70s 465ms/step\n",
      "Loss: 0.20554\n",
      "nuclear iou_score: 0.74159\n",
      "nuclear f1-score: 0.85069\n",
      "models_efficientnetb4_512_320_8_117_2020-03-08 09:16:00.047692\n",
      "hybrid loss\n",
      "150/150 [==============================] - 79s 524ms/step\n",
      "Loss: 0.19156\n",
      "mean iou_score: 0.75981\n",
      "mean f1-score: 0.83806\n",
      "150/150 [==============================] - 78s 523ms/step\n",
      "Loss: 0.19156\n",
      "nuclear iou_score: 0.76146\n",
      "nuclear f1-score: 0.86364\n",
      "models_efficientnetb4_1024_320_8_58_2020-03-08 13:40:57.329230\n",
      "hybrid loss\n",
      "150/150 [==============================] - 70s 463ms/step\n",
      "Loss: 0.18456\n",
      "mean iou_score: 0.76245\n",
      "mean f1-score: 0.83965\n",
      "150/150 [==============================] - 71s 474ms/step\n",
      "Loss: 0.18456\n",
      "nuclear iou_score: 0.76568\n",
      "nuclear f1-score: 0.86643\n",
      "models_efficientnetb4_1500_320_8_40_2020-03-08 18:06:30.098173\n",
      "hybrid loss\n",
      "150/150 [==============================] - 66s 439ms/step\n",
      "Loss: 0.18555\n",
      "mean iou_score: 0.76551\n",
      "mean f1-score: 0.8416\n",
      "150/150 [==============================] - 66s 441ms/step\n",
      "Loss: 0.18555\n",
      "nuclear iou_score: 0.77484\n",
      "nuclear f1-score: 0.8724\n",
      "models_focal_vgg16_1500_640_4_40_2020-03-09 17:32:52.239663\n",
      "focal loss\n",
      "150/150 [==============================] - 53s 354ms/step\n",
      "Loss: 0.0098697\n",
      "mean iou_score: 0.77689\n",
      "mean f1-score: 0.85065\n",
      "150/150 [==============================] - 53s 355ms/step\n",
      "Loss: 0.0098697\n",
      "nuclear iou_score: 0.78513\n",
      "nuclear f1-score: 0.87914\n",
      "models_dice_vgg16_1500_640_4_40_2020-03-09 18:48:40.628932\n",
      "dice loss\n",
      "150/150 [==============================] - 53s 355ms/step\n",
      "Loss: 0.10288\n",
      "mean iou_score: 0.75935\n",
      "mean f1-score: 0.83664\n",
      "150/150 [==============================] - 54s 357ms/step\n",
      "Loss: 0.10288\n",
      "nuclear iou_score: 0.78423\n",
      "nuclear f1-score: 0.87854\n",
      "models_fullval_vgg16_1500_640_4_40_2020-03-09 21:16:35.334652\n",
      "hybrid loss\n",
      "150/150 [==============================] - 58s 383ms/step\n",
      "Loss: 0.17409\n",
      "mean iou_score: 0.77943\n",
      "mean f1-score: 0.84951\n",
      "150/150 [==============================] - 59s 392ms/step\n",
      "Loss: 0.17409\n",
      "nuclear iou_score: 0.80908\n",
      "nuclear f1-score: 0.89401\n",
      "models_fullval_efficientnetb4_1500_320_8_40_2020-03-09 23:10:08.517890\n",
      "hybrid loss\n",
      "150/150 [==============================] - 77s 512ms/step\n",
      "Loss: 0.18412\n",
      "mean iou_score: 0.76643\n",
      "mean f1-score: 0.84292\n",
      "150/150 [==============================] - 76s 510ms/step\n",
      "Loss: 0.18412\n",
      "nuclear iou_score: 0.77488\n",
      "nuclear f1-score: 0.87225\n"
     ]
    }
   ],
   "source": [
    "model_loop = range(14)\n",
    "backbone_loop = ['vgg16','vgg16','vgg16','vgg16','vgg16','efficientnetb4','efficientnetb4','efficientnetb4','efficientnetb4','efficientnetb4','vgg16','vgg16','vgg16','efficientnetb4']\n",
    "loss_loop = ['hybrid','hybrid','hybrid','hybrid','hybrid','hybrid','hybrid','hybrid','hybrid','hybrid','focal','dice','hybrid','hybrid']\n",
    "\n",
    "for mi, nums in enumerate(model_loop):\n",
    "    idx = nums\n",
    "    BACKBONE = backbone_loop[mi]\n",
    "    print(model_filenames[idx])\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "    test_dataset = Dataset(\n",
    "        x_valid_dir, \n",
    "        y_valid_dir, \n",
    "        classes=CLASSES, \n",
    "        augmentation=get_validation_augmentation(),\n",
    "        preprocessing=get_preprocessing(preprocess_input),\n",
    "        n_images = n_images_dev\n",
    "    )\n",
    "\n",
    "    test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    #create model\n",
    "    model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\n",
    "\n",
    "    ### Load model\n",
    "    # model = create_model()\n",
    "    # model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # model = keras.models.load_model(model_dir + '/' + model_filenames[idx])\n",
    "    model.load_weights(model_dir + '/' + model_filenames[idx]+'/best_model_weights.h5')\n",
    "    example_dir = model_dir + '/' + model_filenames[idx] + '/example/'\n",
    "    if not os.path.exists(example_dir):\n",
    "                os.makedirs(example_dir)\n",
    "    \n",
    "    ## predict images        \n",
    "    showimage = 0\n",
    "    saveimage = 0\n",
    "\n",
    "    # n = 5\n",
    "    # ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
    "    #ids = range(4)\n",
    "    # ids = range(10,13)\n",
    "    ids = []\n",
    "\n",
    "    for i in ids:\n",
    "\n",
    "        print(test_dataset.ids[i])\n",
    "\n",
    "        image, gt_mask = test_dataset[i]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        pr_mask = model.predict(image)\n",
    "\n",
    "        if saveimage == 1:\n",
    "            imsave(example_dir + test_dataset.ids[i][0:-4] + '_groundtruth.tif' , (gt_mask[8:2168 , 8:2168 , :] * 65535).astype('uint16'))\n",
    "            #print('Ground truth mask saved.')\n",
    "            imsave(example_dir + test_dataset.ids[i][0:-4] + '_predicted.tif' , (pr_mask[0 , 8:2168 , 8:2168 , :] * 65535).astype('uint16'))\n",
    "            #print('Predicted mask saved.')\n",
    "\n",
    "        if showimage == 1:   \n",
    "            visualize(\n",
    "            image=denormalize(image.squeeze()),\n",
    "            ground_truth_mask=(gt_mask.squeeze()),\n",
    "            #ground_truth_mask=(gt_mask[:,:,1:].squeeze()),\n",
    "            # ground_truth_mask=(gt_mask[:,:,:].squeeze()),\n",
    "\n",
    "            prediction_mask=(pr_mask.squeeze()),\n",
    "            #prediction_mask=(pr_mask[:,:,:,1:].squeeze()),\n",
    "            # prediction_mask=(pr_mask[:,:,:,:].squeeze()),\n",
    "\n",
    "            # background_mask=pr_mask[..., 0].squeeze(),\n",
    "            nucleus_mask=pr_mask[..., 1].squeeze(),\n",
    "            # cytoplasm_mask=pr_mask[..., 2].squeeze(),\n",
    "            # artifact_mask=pr_mask[..., 3].squeeze()\n",
    "        )\n",
    "\n",
    "\n",
    "    # run test set \n",
    "    LR = 0.0001\n",
    "    optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "    # define loss\n",
    "    dice_loss = sm.losses.DiceLoss(class_weights) \n",
    "    focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "    if loss_loop[mi] is 'hybrid':\n",
    "        total_loss = dice_loss + (1 * focal_loss)\n",
    "        print(loss_loop[mi] + ' loss')\n",
    "    elif loss_loop[mi] is 'dice':\n",
    "        total_loss = dice_loss\n",
    "        print(loss_loop[mi] + ' loss')\n",
    "    elif loss_loop[mi] is 'focal':\n",
    "        total_loss = focal_loss\n",
    "        print(loss_loop[mi] + ' loss')\n",
    "    else:\n",
    "        print('error in loss')\n",
    "\n",
    "\n",
    "    # compile keras model with defined optimozer, loss and metrics\n",
    "    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "    model.compile(optim, total_loss, metrics)\n",
    "    scores = model.evaluate_generator(test_dataloader , verbose = 1 , use_multiprocessing = False , workers = 8)\n",
    "    print(\"Loss: {:.5}\".format(scores[0]))\n",
    "    for metric, value in zip(metrics, scores[1:]):\n",
    "        print(\"mean {}: {:.5}\".format(metric.__name__, value))\n",
    "\n",
    "    # for nuclear\n",
    "    class_weights_eval = np.array([0 ,3 , 0 ])\n",
    "    metrics2 = [sm.metrics.IOUScore(threshold=0.5, class_weights = class_weights_eval), sm.metrics.FScore(threshold=0.5, class_weights = class_weights_eval)]\n",
    "    model.compile(optim, total_loss, metrics2)\n",
    "    scores2 = model.evaluate_generator(test_dataloader , verbose = 1 , use_multiprocessing = False , workers = 8)\n",
    "    print(\"Loss: {:.5}\".format(scores[0]))\n",
    "    for metric, value in zip(metrics2, scores2[1:]):\n",
    "        print(\"nuclear {}: {:.5}\".format(metric.__name__, value))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet_segmentation_predict.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
