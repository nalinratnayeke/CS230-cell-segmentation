{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fy_FqY_TDlE6"
   },
   "source": [
    "### Reqirements\n",
    "- keras >= 2.2.0 or tensorflow >= 1.13\n",
    "- segmenation-models==1.0.*\n",
    "- albumentations==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCWcEBobDlFA"
   },
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14593,
     "status": "ok",
     "timestamp": 1583525361063,
     "user": {
      "displayName": "Yilin Fan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKCG-DUc0cADTzFzfUA0pSD74ylP1IlzmQkMuJVQ=s64",
      "userId": "10566861298007020155"
     },
     "user_tz": 480
    },
    "id": "BOSyTNIUDlFC",
    "outputId": "ad95576b-6ef9-4916-f582-18288d811330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKC_AxGnDlFI"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/data/'\n",
    "\n",
    "if not os.path.exists('/data/models'):\n",
    "    os.makedirs('/data/models')\n",
    "\n",
    "# Debug for fileloading\n",
    "FLAG_DEBUG_LOADING = 0\n",
    "\n",
    "# Single z stack\n",
    "FLAG_SINGLE_Z = 1\n",
    "\n",
    "# Convert image to imtype \n",
    "imtype = \"uint8\"\n",
    "# imtype = \"float32\"\n",
    "\n",
    "class_weights=np.array([0 , 1 , .5])\n",
    "class_labels = ['background' , 'nucleus' , 'cytoplasm']\n",
    "\n",
    "\n",
    "dir_tag = ''\n",
    "tag = 'fullval'\n",
    "\n",
    "x_train_dir = [os.path.join(DATA_DIR, 'Image_BF1_train') , os.path.join(DATA_DIR, 'Image_BF2_train') , os.path.join(DATA_DIR, 'Image_BF3_train')]\n",
    "y_train_dir = os.path.join(DATA_DIR, 'Mask_3class_train' + dir_tag)\n",
    "\n",
    "x_valid_dir = [os.path.join(DATA_DIR, 'Image_BF1_dev') , os.path.join(DATA_DIR, 'Image_BF2_dev') , os.path.join(DATA_DIR, 'Image_BF3_dev')]\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'Mask_3class_dev' + dir_tag)\n",
    "\n",
    "x_test_dir = [os.path.join(DATA_DIR, 'Image_BF1_test') , os.path.join(DATA_DIR, 'Image_BF2_test') , os.path.join(DATA_DIR, 'Image_BF3_test')]\n",
    "y_test_dir = os.path.join(DATA_DIR, 'Mask_3class_test' + dir_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GgYAZKGDlFL"
   },
   "source": [
    "# Dataloader and utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2achLqHwDlFM"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, np.ceil(n / 1), i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# classes for data loading and preprocessing\n",
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = class_labels\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            n_images=None\n",
    "    ):\n",
    "        # Make sure files are present in all folders\n",
    "        self.ids1 = [os.path.basename(x) for x in glob.glob(images_dir[1] + '/*.tif')]\n",
    "        self.ids3 = [os.path.basename(x) for x in glob.glob(masks_dir + '/*.tif')]\n",
    "        self.ids = list(set(self.ids1) & set(self.ids3))\n",
    "\n",
    "        # # Number of training examples\n",
    "        if len(self.ids) > n_images:\n",
    "            self.ids = self.ids[0:n_images]\n",
    "\n",
    "        self.images_fps0 = [os.path.join(images_dir[0], image_id) for image_id in self.ids]\n",
    "        self.images_fps1 = [os.path.join(images_dir[1], image_id) for image_id in self.ids]\n",
    "        self.images_fps2 = [os.path.join(images_dir[2], image_id) for image_id in self.ids]\n",
    "\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        if FLAG_DEBUG_LOADING == 1:\n",
    "            print(self.masks_fps[i])\n",
    "            \n",
    "        # read data and convert\n",
    "        if FLAG_SINGLE_Z == 0:\n",
    "            image0 = cv2.imread(self.images_fps0[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image1 = cv2.imread(self.images_fps1[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image2 = cv2.imread(self.images_fps2[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "        else:\n",
    "            image1 = cv2.imread(self.images_fps1[i] , cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "            image0 = image1\n",
    "            image2 = image1\n",
    "        image = np.stack((image0 , image1 , image2) , axis = 2)\n",
    "\n",
    "        if imtype == \"uint8\":\n",
    "            image = cv2.convertScaleAbs(image , alpha = (255. / 65535.))\n",
    "        if imtype == \"float32\":\n",
    "            image = image.astype(\"float32\")\n",
    "\n",
    "        # Resize image\n",
    "        # print(image)\n",
    "        if FLAG_RESIZE == 1:\n",
    "            image = cv2.resize(image , (imsize , imsize))\n",
    "\n",
    "        if FLAG_DEBUG_LOADING == 1:\n",
    "            print(self.masks_fps[i])\n",
    "        mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "        # Resize mask using nearest neighbor to avoid decimal points\n",
    "        if FLAG_RESIZE == 1:\n",
    "            mask = cv2.resize(mask , dsize = (imsize , imsize) , interpolation = cv2.INTER_NEAREST)\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        # print(self.class_values)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary: not necessary since every pixel is labeled\n",
    "        # if mask.shape[-1] != 1:\n",
    "        #     background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "        #     mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)   \n",
    "            \n",
    "            \n",
    "### Augmentations\n",
    "# def round_clip_0_1(x, **kwargs):\n",
    "#     return x.round().clip(0, 1)\n",
    "\n",
    "# define heavy augmentations\n",
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "\n",
    "        A.VerticalFlip(p=0.5),\n",
    "\n",
    "        A.Transpose(p=0.5),\n",
    "\n",
    "        # A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        A.PadIfNeeded(min_height=imsize, min_width=imsize, always_apply=True, border_mode=0),\n",
    "        A.RandomCrop(height=imsize, width=imsize, always_apply=True),\n",
    "\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(imsize_test, imsize_test)\n",
    "    ]\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SFUt_QFDlFd"
   },
   "source": [
    "### Define loop parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loops\n",
    "n_images_loop = np.array([1500])\n",
    "# n_images_loop = np.array([1500])\n",
    "\n",
    "base_epochs = 40\n",
    "epochs_loop = (40*1500/n_images_loop).astype('int')\n",
    "\n",
    "# backbone_loop = ['efficientnetb4','vgg16']\n",
    "# imsize_loop = [320, 640]\n",
    "# batch_loop = [8, 4]\n",
    "\n",
    "backbone_loop = ['vgg16', 'efficientnetb4']\n",
    "imsize_loop = [640, 320]\n",
    "batch_loop = [4, 8]\n",
    "\n",
    "### other stuff\n",
    "# Random crops\n",
    "FLAG_RESIZE = 0\n",
    "LR = 0.0001\n",
    "imsize_test = 2176\n",
    "n_images_dev = 150\n",
    "n_images_test = 150\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4560,
     "status": "ok",
     "timestamp": 1583526310643,
     "user": {
      "displayName": "Yilin Fan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgKCG-DUc0cADTzFzfUA0pSD74ylP1IlzmQkMuJVQ=s64",
      "userId": "10566861298007020155"
     },
     "user_tz": 480
    },
    "id": "BnVwnpXiDlFp",
    "outputId": "2e9da738-8f9d-4fff-cd71-460c13c60e46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 21:16:35.334652\n",
      "Size of training set: 1500\n",
      "Size of training minibatch: (4, 640, 640, 3)\n",
      "Size of training minibatch mask: (4, 640, 640, 3)\n",
      "\n",
      "Size of dev set: 150\n",
      "Size of dev minibatch: (1, 2176, 2176, 3)\n",
      "Size of dev minibatch mask: (1, 2176, 2176, 3)\n",
      "Epoch 1/40\n",
      "375/375 [==============================] - 176s 470ms/step - loss: 0.3417 - iou_score: 0.6141 - f1-score: 0.7408 - acc: 0.7731 - val_loss: 0.2079 - val_iou_score: 0.6515 - val_f1-score: 0.7645 - val_acc: 0.8424\n",
      "Epoch 2/40\n",
      "375/375 [==============================] - 172s 459ms/step - loss: 0.1968 - iou_score: 0.7509 - f1-score: 0.8514 - acc: 0.8718 - val_loss: 0.2088 - val_iou_score: 0.6314 - val_f1-score: 0.7458 - val_acc: 0.7978\n",
      "Epoch 3/40\n",
      "375/375 [==============================] - 167s 445ms/step - loss: 0.1698 - iou_score: 0.7667 - f1-score: 0.8631 - acc: 0.8810 - val_loss: 0.0960 - val_iou_score: 0.7402 - val_f1-score: 0.8247 - val_acc: 0.8852\n",
      "Epoch 4/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1567 - iou_score: 0.7782 - f1-score: 0.8703 - acc: 0.8879 - val_loss: 0.0858 - val_iou_score: 0.7543 - val_f1-score: 0.8340 - val_acc: 0.8938\n",
      "Epoch 5/40\n",
      "375/375 [==============================] - 172s 459ms/step - loss: 0.1524 - iou_score: 0.7776 - f1-score: 0.8696 - acc: 0.8884 - val_loss: 0.0877 - val_iou_score: 0.7522 - val_f1-score: 0.8328 - val_acc: 0.8924\n",
      "Epoch 6/40\n",
      "375/375 [==============================] - 166s 443ms/step - loss: 0.1499 - iou_score: 0.7799 - f1-score: 0.8703 - acc: 0.8915 - val_loss: 0.0835 - val_iou_score: 0.7545 - val_f1-score: 0.8344 - val_acc: 0.8942\n",
      "Epoch 7/40\n",
      "375/375 [==============================] - 166s 441ms/step - loss: 0.1417 - iou_score: 0.7858 - f1-score: 0.8756 - acc: 0.8926 - val_loss: 0.0787 - val_iou_score: 0.7568 - val_f1-score: 0.8354 - val_acc: 0.8938\n",
      "Epoch 8/40\n",
      "375/375 [==============================] - 167s 444ms/step - loss: 0.1444 - iou_score: 0.7814 - f1-score: 0.8715 - acc: 0.8927 - val_loss: 0.1378 - val_iou_score: 0.7144 - val_f1-score: 0.8078 - val_acc: 0.8668\n",
      "Epoch 9/40\n",
      "375/375 [==============================] - 166s 444ms/step - loss: 0.1416 - iou_score: 0.7864 - f1-score: 0.8747 - acc: 0.8941 - val_loss: 0.0781 - val_iou_score: 0.7543 - val_f1-score: 0.8346 - val_acc: 0.8913\n",
      "Epoch 10/40\n",
      "375/375 [==============================] - 167s 445ms/step - loss: 0.1406 - iou_score: 0.7869 - f1-score: 0.8749 - acc: 0.8948 - val_loss: 0.0751 - val_iou_score: 0.7644 - val_f1-score: 0.8406 - val_acc: 0.8986\n",
      "Epoch 11/40\n",
      "375/375 [==============================] - 167s 445ms/step - loss: 0.1361 - iou_score: 0.7891 - f1-score: 0.8764 - acc: 0.8959 - val_loss: 0.0773 - val_iou_score: 0.7631 - val_f1-score: 0.8395 - val_acc: 0.8978\n",
      "Epoch 12/40\n",
      "375/375 [==============================] - 174s 465ms/step - loss: 0.1344 - iou_score: 0.7927 - f1-score: 0.8794 - acc: 0.8967 - val_loss: 0.0892 - val_iou_score: 0.7463 - val_f1-score: 0.8286 - val_acc: 0.8878\n",
      "Epoch 13/40\n",
      "375/375 [==============================] - 166s 443ms/step - loss: 0.1313 - iou_score: 0.7938 - f1-score: 0.8791 - acc: 0.8990 - val_loss: 0.0835 - val_iou_score: 0.7549 - val_f1-score: 0.8342 - val_acc: 0.8934\n",
      "Epoch 14/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1315 - iou_score: 0.7931 - f1-score: 0.8788 - acc: 0.8992 - val_loss: 0.0761 - val_iou_score: 0.7565 - val_f1-score: 0.8353 - val_acc: 0.8933\n",
      "Epoch 15/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1335 - iou_score: 0.7870 - f1-score: 0.8741 - acc: 0.8953 - val_loss: 0.0737 - val_iou_score: 0.7668 - val_f1-score: 0.8419 - val_acc: 0.9001\n",
      "Epoch 16/40\n",
      "375/375 [==============================] - 162s 431ms/step - loss: 0.1339 - iou_score: 0.7909 - f1-score: 0.8762 - acc: 0.8982 - val_loss: 0.0849 - val_iou_score: 0.7507 - val_f1-score: 0.8314 - val_acc: 0.8901\n",
      "Epoch 17/40\n",
      "375/375 [==============================] - 166s 442ms/step - loss: 0.1273 - iou_score: 0.7998 - f1-score: 0.8833 - acc: 0.9011 - val_loss: 0.0847 - val_iou_score: 0.7563 - val_f1-score: 0.8353 - val_acc: 0.8945\n",
      "Epoch 18/40\n",
      "375/375 [==============================] - 167s 446ms/step - loss: 0.1316 - iou_score: 0.7925 - f1-score: 0.8782 - acc: 0.8978 - val_loss: 0.0751 - val_iou_score: 0.7642 - val_f1-score: 0.8403 - val_acc: 0.8984\n",
      "Epoch 19/40\n",
      "375/375 [==============================] - 167s 446ms/step - loss: 0.1301 - iou_score: 0.7925 - f1-score: 0.8788 - acc: 0.8989 - val_loss: 0.0778 - val_iou_score: 0.7687 - val_f1-score: 0.8437 - val_acc: 0.9018\n",
      "Epoch 20/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1304 - iou_score: 0.7963 - f1-score: 0.8800 - acc: 0.9010 - val_loss: 0.0820 - val_iou_score: 0.7577 - val_f1-score: 0.8362 - val_acc: 0.8948\n",
      "Epoch 21/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1270 - iou_score: 0.7991 - f1-score: 0.8825 - acc: 0.9015 - val_loss: 0.0873 - val_iou_score: 0.7466 - val_f1-score: 0.8293 - val_acc: 0.8889\n",
      "Epoch 22/40\n",
      "375/375 [==============================] - 172s 459ms/step - loss: 0.1278 - iou_score: 0.7964 - f1-score: 0.8803 - acc: 0.9002 - val_loss: 0.0739 - val_iou_score: 0.7666 - val_f1-score: 0.8418 - val_acc: 0.8999\n",
      "Epoch 23/40\n",
      "375/375 [==============================] - 166s 442ms/step - loss: 0.1294 - iou_score: 0.7963 - f1-score: 0.8808 - acc: 0.9004 - val_loss: 0.0753 - val_iou_score: 0.7658 - val_f1-score: 0.8413 - val_acc: 0.8992\n",
      "Epoch 24/40\n",
      "375/375 [==============================] - 172s 459ms/step - loss: 0.1279 - iou_score: 0.7969 - f1-score: 0.8814 - acc: 0.9002 - val_loss: 0.0743 - val_iou_score: 0.7695 - val_f1-score: 0.8440 - val_acc: 0.9016\n",
      "Epoch 25/40\n",
      "375/375 [==============================] - 171s 457ms/step - loss: 0.1268 - iou_score: 0.7967 - f1-score: 0.8800 - acc: 0.9014 - val_loss: 0.0797 - val_iou_score: 0.7583 - val_f1-score: 0.8369 - val_acc: 0.8949\n",
      "Epoch 26/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1307 - iou_score: 0.7941 - f1-score: 0.8782 - acc: 0.8993 - val_loss: 0.0805 - val_iou_score: 0.7666 - val_f1-score: 0.8423 - val_acc: 0.9000\n",
      "Epoch 27/40\n",
      "375/375 [==============================] - 171s 455ms/step - loss: 0.1261 - iou_score: 0.7962 - f1-score: 0.8801 - acc: 0.9005 - val_loss: 0.0729 - val_iou_score: 0.7645 - val_f1-score: 0.8409 - val_acc: 0.8977\n",
      "Epoch 28/40\n",
      "375/375 [==============================] - 166s 442ms/step - loss: 0.1257 - iou_score: 0.8008 - f1-score: 0.8837 - acc: 0.9024 - val_loss: 0.0717 - val_iou_score: 0.7773 - val_f1-score: 0.8493 - val_acc: 0.9058\n",
      "Epoch 29/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1224 - iou_score: 0.8032 - f1-score: 0.8856 - acc: 0.9033 - val_loss: 0.0689 - val_iou_score: 0.7741 - val_f1-score: 0.8468 - val_acc: 0.9041\n",
      "Epoch 30/40\n",
      "375/375 [==============================] - 172s 460ms/step - loss: 0.1264 - iou_score: 0.7962 - f1-score: 0.8796 - acc: 0.9011 - val_loss: 0.0718 - val_iou_score: 0.7678 - val_f1-score: 0.8425 - val_acc: 0.8999\n",
      "Epoch 31/40\n",
      "375/375 [==============================] - 170s 454ms/step - loss: 0.1268 - iou_score: 0.8004 - f1-score: 0.8843 - acc: 0.9002 - val_loss: 0.0763 - val_iou_score: 0.7609 - val_f1-score: 0.8380 - val_acc: 0.8965\n",
      "Epoch 32/40\n",
      "375/375 [==============================] - 172s 457ms/step - loss: 0.1246 - iou_score: 0.7977 - f1-score: 0.8812 - acc: 0.9018 - val_loss: 0.0731 - val_iou_score: 0.7727 - val_f1-score: 0.8460 - val_acc: 0.9031\n",
      "Epoch 33/40\n",
      "375/375 [==============================] - 172s 458ms/step - loss: 0.1228 - iou_score: 0.8018 - f1-score: 0.8841 - acc: 0.9032 - val_loss: 0.0669 - val_iou_score: 0.7795 - val_f1-score: 0.8502 - val_acc: 0.9069\n",
      "Epoch 34/40\n",
      "375/375 [==============================] - 171s 456ms/step - loss: 0.1256 - iou_score: 0.7956 - f1-score: 0.8788 - acc: 0.9008 - val_loss: 0.0759 - val_iou_score: 0.7685 - val_f1-score: 0.8435 - val_acc: 0.9009\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 35/40\n",
      "375/375 [==============================] - 167s 445ms/step - loss: 0.1238 - iou_score: 0.8044 - f1-score: 0.8858 - acc: 0.9048 - val_loss: 0.0731 - val_iou_score: 0.7732 - val_f1-score: 0.8460 - val_acc: 0.9035\n",
      "Epoch 36/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 166s 443ms/step - loss: 0.1189 - iou_score: 0.8077 - f1-score: 0.8880 - acc: 0.9064 - val_loss: 0.0707 - val_iou_score: 0.7712 - val_f1-score: 0.8451 - val_acc: 0.9027\n",
      "Epoch 37/40\n",
      "375/375 [==============================] - 163s 434ms/step - loss: 0.1173 - iou_score: 0.8091 - f1-score: 0.8900 - acc: 0.9064 - val_loss: 0.0699 - val_iou_score: 0.7771 - val_f1-score: 0.8491 - val_acc: 0.9053\n",
      "Epoch 38/40\n",
      "375/375 [==============================] - 171s 456ms/step - loss: 0.1167 - iou_score: 0.8059 - f1-score: 0.8868 - acc: 0.9058 - val_loss: 0.0726 - val_iou_score: 0.7735 - val_f1-score: 0.8466 - val_acc: 0.9038\n",
      "Epoch 39/40\n",
      "375/375 [==============================] - 167s 446ms/step - loss: 0.1166 - iou_score: 0.8086 - f1-score: 0.8878 - acc: 0.9079 - val_loss: 0.0683 - val_iou_score: 0.7792 - val_f1-score: 0.8505 - val_acc: 0.9066\n",
      "Epoch 40/40\n",
      "375/375 [==============================] - 161s 429ms/step - loss: 0.1176 - iou_score: 0.8083 - f1-score: 0.8881 - acc: 0.9072 - val_loss: 0.0760 - val_iou_score: 0.7669 - val_f1-score: 0.8419 - val_acc: 0.9008\n",
      "2020-03-09 23:10:08.517890\n",
      "Size of training set: 1500\n",
      "Size of training minibatch: (8, 320, 320, 3)\n",
      "Size of training minibatch mask: (8, 320, 320, 3)\n",
      "\n",
      "Size of dev set: 150\n",
      "Size of dev minibatch: (1, 2176, 2176, 3)\n",
      "Size of dev minibatch mask: (1, 2176, 2176, 3)\n",
      "Epoch 1/40\n",
      "187/187 [==============================] - 225s 1s/step - loss: 0.3674 - iou_score: 0.5742 - f1-score: 0.7005 - acc: 0.7490 - val_loss: 0.2192 - val_iou_score: 0.6786 - val_f1-score: 0.7825 - val_acc: 0.8501\n",
      "Epoch 2/40\n",
      "187/187 [==============================] - 173s 927ms/step - loss: 0.2246 - iou_score: 0.7342 - f1-score: 0.8415 - acc: 0.8569 - val_loss: 0.1194 - val_iou_score: 0.7178 - val_f1-score: 0.8110 - val_acc: 0.8712\n",
      "Epoch 3/40\n",
      "187/187 [==============================] - 175s 935ms/step - loss: 0.1844 - iou_score: 0.7617 - f1-score: 0.8618 - acc: 0.8733 - val_loss: 0.1095 - val_iou_score: 0.7266 - val_f1-score: 0.8163 - val_acc: 0.8758\n",
      "Epoch 4/40\n",
      "187/187 [==============================] - 166s 886ms/step - loss: 0.1790 - iou_score: 0.7655 - f1-score: 0.8637 - acc: 0.8776 - val_loss: 0.1075 - val_iou_score: 0.7304 - val_f1-score: 0.8184 - val_acc: 0.8805\n",
      "Epoch 5/40\n",
      "187/187 [==============================] - 161s 862ms/step - loss: 0.1731 - iou_score: 0.7674 - f1-score: 0.8639 - acc: 0.8798 - val_loss: 0.0925 - val_iou_score: 0.7379 - val_f1-score: 0.8239 - val_acc: 0.8829\n",
      "Epoch 6/40\n",
      "187/187 [==============================] - 162s 866ms/step - loss: 0.1621 - iou_score: 0.7741 - f1-score: 0.8698 - acc: 0.8830 - val_loss: 0.0938 - val_iou_score: 0.7420 - val_f1-score: 0.8270 - val_acc: 0.8849\n",
      "Epoch 7/40\n",
      "187/187 [==============================] - 174s 929ms/step - loss: 0.1546 - iou_score: 0.7786 - f1-score: 0.8731 - acc: 0.8845 - val_loss: 0.0913 - val_iou_score: 0.7441 - val_f1-score: 0.8286 - val_acc: 0.8860\n",
      "Epoch 8/40\n",
      "187/187 [==============================] - 173s 924ms/step - loss: 0.1549 - iou_score: 0.7753 - f1-score: 0.8703 - acc: 0.8832 - val_loss: 0.0879 - val_iou_score: 0.7454 - val_f1-score: 0.8284 - val_acc: 0.8877\n",
      "Epoch 9/40\n",
      "187/187 [==============================] - 164s 875ms/step - loss: 0.1526 - iou_score: 0.7795 - f1-score: 0.8725 - acc: 0.8859 - val_loss: 0.0848 - val_iou_score: 0.7534 - val_f1-score: 0.8340 - val_acc: 0.8925\n",
      "Epoch 10/40\n",
      "187/187 [==============================] - 161s 859ms/step - loss: 0.1518 - iou_score: 0.7809 - f1-score: 0.8729 - acc: 0.8878 - val_loss: 0.0913 - val_iou_score: 0.7481 - val_f1-score: 0.8308 - val_acc: 0.8901\n",
      "Epoch 11/40\n",
      "187/187 [==============================] - 165s 882ms/step - loss: 0.1477 - iou_score: 0.7815 - f1-score: 0.8738 - acc: 0.8873 - val_loss: 0.0813 - val_iou_score: 0.7547 - val_f1-score: 0.8355 - val_acc: 0.8923\n",
      "Epoch 12/40\n",
      "187/187 [==============================] - 166s 889ms/step - loss: 0.1454 - iou_score: 0.7837 - f1-score: 0.8762 - acc: 0.8880 - val_loss: 0.0816 - val_iou_score: 0.7519 - val_f1-score: 0.8327 - val_acc: 0.8912\n",
      "Epoch 13/40\n",
      "187/187 [==============================] - 176s 943ms/step - loss: 0.1457 - iou_score: 0.7832 - f1-score: 0.8749 - acc: 0.8892 - val_loss: 0.0855 - val_iou_score: 0.7492 - val_f1-score: 0.8311 - val_acc: 0.8899\n",
      "Epoch 14/40\n",
      "187/187 [==============================] - 166s 888ms/step - loss: 0.1429 - iou_score: 0.7841 - f1-score: 0.8761 - acc: 0.8884 - val_loss: 0.0828 - val_iou_score: 0.7547 - val_f1-score: 0.8347 - val_acc: 0.8936\n",
      "Epoch 15/40\n",
      "187/187 [==============================] - 172s 918ms/step - loss: 0.1437 - iou_score: 0.7840 - f1-score: 0.8754 - acc: 0.8891 - val_loss: 0.0880 - val_iou_score: 0.7494 - val_f1-score: 0.8316 - val_acc: 0.8905\n",
      "Epoch 16/40\n",
      "187/187 [==============================] - 165s 881ms/step - loss: 0.1449 - iou_score: 0.7800 - f1-score: 0.8724 - acc: 0.8889 - val_loss: 0.0851 - val_iou_score: 0.7579 - val_f1-score: 0.8386 - val_acc: 0.8943\n",
      "Epoch 17/40\n",
      "187/187 [==============================] - 168s 896ms/step - loss: 0.1418 - iou_score: 0.7825 - f1-score: 0.8743 - acc: 0.8887 - val_loss: 0.0856 - val_iou_score: 0.7517 - val_f1-score: 0.8337 - val_acc: 0.8908\n",
      "Epoch 18/40\n",
      "187/187 [==============================] - 167s 896ms/step - loss: 0.1408 - iou_score: 0.7863 - f1-score: 0.8761 - acc: 0.8922 - val_loss: 0.0789 - val_iou_score: 0.7581 - val_f1-score: 0.8375 - val_acc: 0.8944\n",
      "Epoch 19/40\n",
      "187/187 [==============================] - 162s 867ms/step - loss: 0.1393 - iou_score: 0.7893 - f1-score: 0.8786 - acc: 0.8922 - val_loss: 0.0862 - val_iou_score: 0.7531 - val_f1-score: 0.8340 - val_acc: 0.8935\n",
      "Epoch 20/40\n",
      "187/187 [==============================] - 167s 895ms/step - loss: 0.1410 - iou_score: 0.7850 - f1-score: 0.8750 - acc: 0.8903 - val_loss: 0.0903 - val_iou_score: 0.7455 - val_f1-score: 0.8291 - val_acc: 0.8890\n",
      "Epoch 21/40\n",
      "187/187 [==============================] - 167s 892ms/step - loss: 0.1361 - iou_score: 0.7917 - f1-score: 0.8811 - acc: 0.8927 - val_loss: 0.0806 - val_iou_score: 0.7607 - val_f1-score: 0.8402 - val_acc: 0.8959\n",
      "Epoch 22/40\n",
      "187/187 [==============================] - 166s 887ms/step - loss: 0.1377 - iou_score: 0.7915 - f1-score: 0.8804 - acc: 0.8929 - val_loss: 0.0747 - val_iou_score: 0.7577 - val_f1-score: 0.8363 - val_acc: 0.8946\n",
      "Epoch 23/40\n",
      "187/187 [==============================] - 173s 926ms/step - loss: 0.1400 - iou_score: 0.7911 - f1-score: 0.8795 - acc: 0.8934 - val_loss: 0.0763 - val_iou_score: 0.7617 - val_f1-score: 0.8396 - val_acc: 0.8969\n",
      "Epoch 24/40\n",
      "187/187 [==============================] - 162s 866ms/step - loss: 0.1402 - iou_score: 0.7885 - f1-score: 0.8779 - acc: 0.8926 - val_loss: 0.0790 - val_iou_score: 0.7577 - val_f1-score: 0.8372 - val_acc: 0.8943\n",
      "Epoch 25/40\n",
      "187/187 [==============================] - 166s 887ms/step - loss: 0.1343 - iou_score: 0.7917 - f1-score: 0.8808 - acc: 0.8918 - val_loss: 0.0807 - val_iou_score: 0.7568 - val_f1-score: 0.8370 - val_acc: 0.8940\n",
      "Epoch 26/40\n",
      "187/187 [==============================] - 166s 890ms/step - loss: 0.1351 - iou_score: 0.7915 - f1-score: 0.8800 - acc: 0.8937 - val_loss: 0.0796 - val_iou_score: 0.7564 - val_f1-score: 0.8362 - val_acc: 0.8936\n",
      "Epoch 27/40\n",
      "187/187 [==============================] - 164s 875ms/step - loss: 0.1390 - iou_score: 0.7874 - f1-score: 0.8765 - acc: 0.8935 - val_loss: 0.0717 - val_iou_score: 0.7672 - val_f1-score: 0.8441 - val_acc: 0.8987\n",
      "Epoch 28/40\n",
      "187/187 [==============================] - 174s 931ms/step - loss: 0.1409 - iou_score: 0.7872 - f1-score: 0.8769 - acc: 0.8912 - val_loss: 0.0776 - val_iou_score: 0.7543 - val_f1-score: 0.8341 - val_acc: 0.8924\n",
      "Epoch 29/40\n",
      "187/187 [==============================] - 171s 914ms/step - loss: 0.1329 - iou_score: 0.7968 - f1-score: 0.8845 - acc: 0.8959 - val_loss: 0.0786 - val_iou_score: 0.7645 - val_f1-score: 0.8423 - val_acc: 0.8981\n",
      "Epoch 30/40\n",
      "187/187 [==============================] - 166s 887ms/step - loss: 0.1315 - iou_score: 0.7986 - f1-score: 0.8853 - acc: 0.8966 - val_loss: 0.0753 - val_iou_score: 0.7647 - val_f1-score: 0.8419 - val_acc: 0.8982\n",
      "Epoch 31/40\n",
      "187/187 [==============================] - 168s 898ms/step - loss: 0.1323 - iou_score: 0.7995 - f1-score: 0.8857 - acc: 0.8976 - val_loss: 0.0808 - val_iou_score: 0.7604 - val_f1-score: 0.8392 - val_acc: 0.8963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "187/187 [==============================] - 165s 881ms/step - loss: 0.1342 - iou_score: 0.7929 - f1-score: 0.8823 - acc: 0.8931 - val_loss: 0.0815 - val_iou_score: 0.7615 - val_f1-score: 0.8401 - val_acc: 0.8973\n",
      "Epoch 33/40\n",
      "187/187 [==============================] - 165s 885ms/step - loss: 0.1370 - iou_score: 0.7907 - f1-score: 0.8791 - acc: 0.8932 - val_loss: 0.0800 - val_iou_score: 0.7622 - val_f1-score: 0.8400 - val_acc: 0.8972\n",
      "Epoch 34/40\n",
      "187/187 [==============================] - 166s 889ms/step - loss: 0.1335 - iou_score: 0.7930 - f1-score: 0.8813 - acc: 0.8952 - val_loss: 0.0759 - val_iou_score: 0.7682 - val_f1-score: 0.8453 - val_acc: 0.8997\n",
      "Epoch 35/40\n",
      "187/187 [==============================] - 164s 877ms/step - loss: 0.1351 - iou_score: 0.7903 - f1-score: 0.8803 - acc: 0.8926 - val_loss: 0.0837 - val_iou_score: 0.7604 - val_f1-score: 0.8396 - val_acc: 0.8959\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 36/40\n",
      "187/187 [==============================] - 171s 916ms/step - loss: 0.1327 - iou_score: 0.7973 - f1-score: 0.8841 - acc: 0.8975 - val_loss: 0.0762 - val_iou_score: 0.7696 - val_f1-score: 0.8452 - val_acc: 0.9012\n",
      "Epoch 37/40\n",
      "187/187 [==============================] - 165s 885ms/step - loss: 0.1319 - iou_score: 0.7948 - f1-score: 0.8829 - acc: 0.8960 - val_loss: 0.0778 - val_iou_score: 0.7672 - val_f1-score: 0.8438 - val_acc: 0.8997\n",
      "Epoch 38/40\n",
      "187/187 [==============================] - 161s 859ms/step - loss: 0.1313 - iou_score: 0.7972 - f1-score: 0.8834 - acc: 0.8973 - val_loss: 0.0739 - val_iou_score: 0.7700 - val_f1-score: 0.8458 - val_acc: 0.9011\n",
      "Epoch 39/40\n",
      "187/187 [==============================] - 168s 897ms/step - loss: 0.1295 - iou_score: 0.7981 - f1-score: 0.8845 - acc: 0.8979 - val_loss: 0.0760 - val_iou_score: 0.7676 - val_f1-score: 0.8441 - val_acc: 0.8998\n",
      "Epoch 40/40\n",
      "187/187 [==============================] - 168s 896ms/step - loss: 0.1318 - iou_score: 0.7962 - f1-score: 0.8841 - acc: 0.8957 - val_loss: 0.0747 - val_iou_score: 0.7711 - val_f1-score: 0.8474 - val_acc: 0.9014\n"
     ]
    }
   ],
   "source": [
    "for ind, BACKBONE in enumerate(backbone_loop):\n",
    "    imsize = imsize_loop[ind]\n",
    "\n",
    "    for ind2, n_images_train in enumerate(n_images_loop):\n",
    "        EPOCHS = epochs_loop[ind2]\n",
    "\n",
    "        preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "        \n",
    "        # define network parameters\n",
    "        n_classes = 1 if len(class_labels) == 1 else (len(class_labels))  # case for binary and multiclass segmentation\n",
    "        activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "        #create model\n",
    "        model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\n",
    "\n",
    "        # define optomizer\n",
    "        optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "        # Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "        # set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "        # dice_loss = sm.losses.DiceLoss() \n",
    "\n",
    "        dice_loss = sm.losses.DiceLoss(class_weights) \n",
    "        focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "        total_loss = dice_loss + (1 * focal_loss)\n",
    "        #total_loss = dice_loss\n",
    "\n",
    "        # actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "        # total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "        metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), 'acc']\n",
    "\n",
    "        # compile keras model with defined optimozer, loss and metrics\n",
    "        model.compile(optim, total_loss, metrics)\n",
    "        # Dataset for train images\n",
    "        from datetime import datetime\n",
    "        # current date and time\n",
    "        now = datetime.now()\n",
    "        print(str(now))\n",
    "\n",
    "        train_dataset = Dataset(\n",
    "            x_train_dir, \n",
    "            y_train_dir, \n",
    "            classes=class_labels, \n",
    "            augmentation=get_training_augmentation(),\n",
    "            preprocessing=get_preprocessing(preprocess_input),\n",
    "            n_images = n_images_train\n",
    "        )\n",
    "\n",
    "        # Dataset for validation images\n",
    "        valid_dataset = Dataset(\n",
    "            x_valid_dir, \n",
    "            y_valid_dir, \n",
    "            classes=class_labels, \n",
    "            augmentation=get_validation_augmentation(),\n",
    "            preprocessing=get_preprocessing(preprocess_input),\n",
    "            n_images = n_images_dev\n",
    "        )\n",
    "        valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # check shapes for errors\n",
    "        assert train_dataloader[0][0].shape == (BATCH_SIZE, imsize, imsize, 3)\n",
    "        assert train_dataloader[0][1].shape == (BATCH_SIZE, imsize, imsize, n_classes)\n",
    "\n",
    "        print('Size of training set: ' + str(len(train_dataset)))\n",
    "        print('Size of training minibatch: ' + str(train_dataloader[0][0].shape))\n",
    "        print('Size of training minibatch mask: ' + str(train_dataloader[0][1].shape))\n",
    "        print('')\n",
    "        print('Size of dev set: ' + str(len(valid_dataset)))\n",
    "        print('Size of dev minibatch: ' + str(valid_dataloader[0][0].shape))\n",
    "        print('Size of dev minibatch mask: ' + str(valid_dataloader[0][1].shape))\n",
    "\n",
    "\n",
    "           \n",
    "        model_save_foldername = '/data/models/models_' + tag + '_' + BACKBONE + '_' + str(n_images_train) + '_' + str(imsize) + '_' + str(BATCH_SIZE)  + '_' + str(EPOCHS) + '_' + str(now)\n",
    "        model_save_filename = model_save_foldername + '/' + 'best_model_weights.h5'\n",
    "        if not os.path.exists(model_save_foldername):\n",
    "            os.makedirs(model_save_foldername)\n",
    "        # model_save_filename = model_save_foldername + '/' + 'weights.epoch{epoch:02d}-loss:{loss:.4f}-f1:{f1-score:.4f}-iou:{iou_score:.4f}-accuracy:{acc:.4f}-val_loss:{val_loss:.4f}-val_f1:{val_f1-score:.4f}-val_iou:{val_iou_score:.4f}-val_accuracy:{val_acc:.4f}' + '.h5'\n",
    "        callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(model_save_filename, monitor = 'val_loss', save_weights_only=True, save_best_only=True, mode='min', period = 1),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr = LR/16, verbose = 1),\n",
    "            keras.callbacks.callbacks.CSVLogger(model_save_foldername + '/' + 'traininglog', separator=',', append=False)\n",
    "        ]\n",
    "\n",
    "        # train model\n",
    "        history = model.fit_generator(\n",
    "            train_dataloader, \n",
    "            steps_per_epoch=len(train_dataloader), \n",
    "            epochs=EPOCHS, \n",
    "            callbacks=callbacks, \n",
    "            validation_data=valid_dataloader, \n",
    "            validation_steps=len(valid_dataloader),\n",
    "            workers = 16,\n",
    "            use_multiprocessing=False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "unet_segmentation_1z_instance0_vgg16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
